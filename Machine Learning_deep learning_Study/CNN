#패션 MNIST 데이터 불러오기
from tensorflow import keras
from sklearn.model_selection import train_test_split

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

train_scaled = train_input.reshape(-1, 28, 28, 1) / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size = 0.2, random_state = 42)

#첫 번째 합성곱 층 만들기

model = keras.Sequential()
model.add(keras.layers.Conv2D(32, kernel_size = 3, activation = 'relu', padding = 'same', input_shape = (28, 28, 1)))

#첫 번째 풀링 층 추가
model.add(keras.layers.MaxPooling2D(2))

#두 번째 합성곱 층 만들기
model.add(keras.layers.Conv2D(64, kernel_size = 3, activation = 'relu', padding = 'same'))

#두 번째 풀링 층 추가
model.add(keras.layers.MaxPooling2D(2))

#특성맵 일렬로 펼치기
model.add(keras.layers.Flatten())

#밀집 은닉층 추가
model.add(keras.layers.Dense(100, activation = 'relu'))

#드롭아웃 추가
model.add(keras.layers.Dropout(0.4))

#출력층 추가
model.add(keras.layers.Dense(10, activation = 'softmax'))

#모델 구조 출력
model.summary()

#층의 구성을 그림으로 표현
keras.utils.plot_model(model)

#그림에 입력과 출력의 크기 표시
keras.utils.plot_model(model, show_shapes = True)

#모델 컴파일
model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')

#ModelCheckpoint 콜백
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-cnn-model.h5', save_best_only = True)

#EarlyStopping 콜백
early_stopping_cb = keras.callbacks.EarlyStopping(patience = 2, restore_best_weights = True)

#모델 훈련
history = model.fit(train_scaled, train_target, epochs = 20, validation_data = (val_scaled, val_target), callbacks = [checkpoint_cb, early_stopping_cb])

#손실 그래프
import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()

#성능평가
model.evaluate(val_scaled, val_target)

#첫 번째 샘플 이미지
plt.imshow(val_scaled[0].reshape(28,28), cmap='gray_r')
plt.show()

#첫 번째 샘플 이미지에 대한 예측 확률 출력
preds = model.predict(val_scaled[0:1])
print(preds)

#확률을 막대 그래프로 표현
plt.bar(range(10), preds[0])

plt.xlabel('class')
plt.ylabel('prob.')
plt.show()

classes = ['티셔츠', '바지', '스웨터', '드레스', '코트', '샌달', '셔츠', '스니커즈', '가방', '앵클 부츠']

#classes를 이용해 결과 확인
import numpy as np

print(classes[np.argmax(preds)])

#합성곱 신경망의 일반화 성능
test_scaled = test_input.reshape(-1, 28, 28, 1) / 255.0
model.evaluate(test_scaled, test_target)

#합성곱 신경망 시각화
#모델 불러오기
from tensorflow import keras

model = keras.models.load_model('best-cnn-model.h5')

#케라스 모델에 추가한 층 확인
model.layers

#합성곱 층의 가중치와 절편의 크기 확인
conv = model.layers[0]
print(conv.weights[0].shape, conv.weights[1].shape)

#가중치의 평균값과 표준편차 구하기
conv_weights = conv.weights[0].numpy()
print(conv_weights.mean(), conv_weights.std())

#히스토그램 그리기
import matplotlib.pyplot as plt

plt.hist(conv_weights.reshape(-1, 1))
plt.xlabel('weight')
plt.ylabel('count')
plt.show()

#커널 출력
fig, axs = plt.subplots(2, 16, figsize = (15, 2))
for i in range(2):
  for j in range(16):
    axs[i, j].imshow(conv_weights[:, :, 0, i * 16 + j], vmin = -0.5, vmax = 0.5)
    axs[i, j].axis('off')
plt.show()

#훈련하지 않은 합성곱 신경망
no_training_model = keras.Sequential()
no_training_model.add(keras.layers.Conv2D(32, kernel_size = 3, activation = 'relu', padding = 'same', input_shape = (28, 28, 1)))

#가중치를 저장
no_training_conv = no_training_model.layers[0]
print(no_training_conv.weights[0].shape)

#훈련하지 않은 합성곱 층의 가중치의 평균값과 표준편차 구하기
no_training_weights = no_training_conv.weights[0].numpy()
print(no_training_weights.mean(), no_training_weights.std())

#히스토그램 그리기
plt.hist(no_training_weights.reshape(-1, 1))
plt.xlabel('weight')
plt.ylabel('count')
plt.show()

#커널 출력
fig, axs = plt.subplots(2, 16, figsize = (15, 2))
for i in range(2):
  for j in range(16):
    axs[i, j].imshow(no_training_weights[:, :, 0, i * 16 + j], vmin = -0.5, vmax = 0.5)
    axs[i, j].axis('off')
plt.show()

#합성곱 층을 함수형 API로 구현
inputs = keras.Input(shape=(784,))
dense1 = keras.layers.Dense(100, activation = 'sigmoid')
dense2 = keras.layers.Dense(10, activation = 'softmax')

#함수 호출
hidden = dense1(inputs)
outputs = dense2(hidden)

#inputs와 outputs을 model 클래스로 연결
model1 = keras.Model(inputs = inputs, outputs = outputs)

#모델의 입력 출력
print(model.input)

#model1.input과 model1.layers[0].output을 연결
#첫 번째 합성곱 층
conv_acti = keras.Model(model.input, model.layers[0].output)

#첫 번째 샘플
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()
plt.imshow(train_input[0], cmap = 'gray_r')
plt.show()

inputs = train_input[0:1].reshape(-1, 28, 28, 1) / 255.0
feature_maps = conv_acti.predict(inputs)

#feature_maps 크기
print(feature_maps.shape)

fig, axs = plt.subplots(4, 8, figsize = (15, 8))
for i in range(4):
    for j in range(8):
        axs[i, j].imshow(feature_maps[0, :, :, i * 8 + j])
        axs[i, j].axis('off')
plt.show()

#두 번째 합성곱 층
conv2_acti = keras.Model(model.input, model.layers[2].output)

inputs = train_input[0:1].reshape(-1, 28, 28, 1) / 255.0
feature_maps = conv2_acti.predict(inputs)

#특성맵 크기
print(feature_maps.shape)

#특성맵 그리기
fig, axs = plt.subplots(8, 8, figsize = (12, 12))
for i in range(8):
    for j in range(8):
        axs[i, j].imshow(feature_maps[0, :, :, i * 8 + j])
        axs[i, j].axis('off')

plt.show()

