#데이터 불러오기
from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

from sklearn.model_selection import train_test_split

#이미지 픽셀값을 0~255dptj 0~1 사이로 변환
train_scaled = train_input / 255.0

#28*28 크기의 2차원 배열을 784 크기의 1차원 배열 변환
train_scaled = train_scaled.reshape(-1, 28*28)

#훈련 세트와 검증 세트로 분할
train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size = 0.2, random_state = 42)

#은닉층
dense1 = keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,))

#출력층
dense2 = keras.layers.Dense(10, activation = 'softmax')

#심층 신경망(DNN)
model = keras.Sequential([dense1, dense2])

#층에 대한 정보
model.summary()

#Sequential 클래스에 층을 추가하는 다른 방법1
model = keras.Sequential([
    keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,),
                       name = 'hidden'),
    keras.layers.Dense(10, activation = 'softmax', name = 'output')
], name = '패션 MNIST 모델')

model.summary()

#Sequential 클래스에 층을 추가하는 다른 방법2
model = keras.Sequential()
model.add(keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,)))
model.add(keras.layers.Dense(10, activation = 'softmax'))

model.summary()

#모델 훈련
model.compile(loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')
model.fit(train_scaled, train_target, epochs=5)

#렐루 함수
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape = (28,28)))
model.add(keras.layers.Dense(100, activation = 'relu'))
model.add(keras.layers.Dense(10, activation = 'softmax'))

model.summary()

#다시 훈련
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

train_scaled = train_input / 255.0
train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size = 0.2, random_state = 42)

#모델 컴파일 후 훈련
model.compile(loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')
model.fit(train_scaled, train_target, epochs = 5)

#성능 확인
model.evaluate(val_scaled, val_target)

#여러가지 옵티마이저 테스트 (SGD 옵티마이저)
model.compile(optimizer = 'sgd', loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')

#여러가지 옵티마이저 테스트 (SGD 옵티마이저_2)
sgd = keras.optimizers.SGD()

#학습률 변경
#sgd = keras.optimizers.SGD(learning_rate = 0.1)

model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')

#여러가지 옵티마이저 테스트 (네스테로프 모멘텀 최적화)
sgd = keras.optimizers.SGD(momentum = 0.9, nesterov=True)

#여러가지 옵티마이저 테스트 (적응적 학습률 Adagrad)
adagrad = keras.optimizers.Adagrad()
model.compile(optimizer = adagrad, loss = 'sparse_categorical_crossentropy', metrics = 'acciracy')

#여러가지 옵티마이저 테스트 (적응적 학습률 RMSprop)
rmsprop = keras.optimizers.RMSprop()
model.compile(optimizer = rmsprop, loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')

#옵티마이저(Adam)으로 다시 훈련
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28, 28)))
model.add(keras.layers.Dense(100, activation = 'relu'))
model.add(keras.layers.Dense(10, activation = 'softmax'))

model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')
model.fit(train_scaled, train_target, epochs = 5)

#검증 세트에서의 성능
model.evaluate(val_scaled, val_target)
