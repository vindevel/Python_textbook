#IMDB 리뷰 데이터셋 불러오기
from tensorflow.keras.datasets import imdb

(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words = 300)

#훈련세트와 테스트 세트의 크기 확인

print(train_input.shape, test_input.shape)

#첫 번째 리뷰의 길이
print(len(train_input[0]))

#두 번째 리뷰의 길이
print(len(train_input[1]))

#첫 번째 리뷰에 닮긴 내용
print(train_input[0])

#타깃 데이터 출력
print(train_target[:20])

#훈련 세트와 검증 세트 분리
from sklearn.model_selection import train_test_split

train_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size = 0.2, random_state = 42)

#평균적인 리뷰의 길이와 가장 짧은 리뷰의 길이 확인
import numpy as np

lengths = np.array([len(x) for x in train_input])
print(np.mean(lengths), np.median(lengths))

#lengths 배열 히스토그램 표현
import matplotlib.pyplot as plt

plt.hist(lengths)
plt.xlabel('length')
plt.ylabel('frequency')
plt.show()

#리뷰 길이가 100이 되도록 수정
from tensorflow.keras.preprocessing.sequence import pad_sequences

train_seq = pad_sequences(train_input, maxlen = 100)

#train_seq 길이 확인
print(train_seq.shape)

#train_seq에 있는 첫 번째 샘플을 출력
print(train_seq[0])

#원본 샘플의 끝을 확인
print(train_input[0][-10:])     

#train_seq에 있는 여섯 번째 샘플
print(train_seq[5])

#검증 세트 길이도 100으로 맞춘다
val_seq = pad_sequences(val_input, maxlen = 100)

#순환신경망 만들기
from tensorflow import keras

model = keras.Sequential()
model.add(keras.layers.SimpleRNN(8, input_shape = (100, 300)))
model.add(keras.layers.Dense(1, activation = 'sigmoid'))
                    
#원-핫 인코딩
train_oh = keras.utils.to_categorical(train_seq)
print(train_oh.shape)

#train_oh 프린트
print(train_oh[0][0][:12])

#val_seq도 원-핫 인코딩
val_oh = keras.utils.to_categorical(val_seq)

model.summary()

#순환신경망 훈련
rmsprop = keras.optimizers.RMSprop(learning_rate = 1e-4)

model.compile(optimizer = rmsprop, loss = 'binary_crossentropy',metrics = ['accuracy'])

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.keras', save_best_only = True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)

history = model.fit(train_oh, train_target, epochs = 100, batch_size = 64,
                    validation_data = (val_oh, val_target),
                    callbacks = [checkpoint_cb, early_stopping_cb])

#훈련손실, 검증손실 그래프
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()

#데이터 크기 확인
print(train_seq.nbytes, train_oh.nbytes)

#단어 임베딩을 사용하여 두 번째 순환신경망 만들기
model2 = keras.Sequential()
model2.add(keras.layers.Embedding(300, 16, input_length = 100))
model2.add(keras.layers.SimpleRNN(8))
model2.add(keras.layers.Dense(1, activation = 'sigmoid'))

model2.summary()

#모델 훈련
#순환신경망 훈련
rmsprop = keras.optimizers.RMSprop(learning_rate = 1e-4)

model2.compile(optimizer = rmsprop, loss = 'binary_crossentropy',metrics = ['accuracy'])

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-embedding-model.h5', save_best_only = True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)

history = model2.fit(train_seq, train_target, epochs = 100, batch_size = 64,
                    validation_data = (val_seq, val_target),
                    callbacks = [checkpoint_cb, early_stopping_cb])

#훈련손실, 검증손실 그래프
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
